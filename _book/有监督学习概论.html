<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 5 有监督学习概论 | 数据科学导论（Introductory of Data Science）</title>

    <meta name="author" content="潘蓄林" />
  
   <meta name="description" content="2022年数据科学导论" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 5 有监督学习概论 | 数据科学导论（Introductory of Data Science）" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="2022年数据科学导论" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 有监督学习概论 | 数据科学导论（Introductory of Data Science）" />
  
  <meta name="twitter:description" content="2022年数据科学导论" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.3.1/transition.js"></script>
    <script src="libs/bs3compat-0.3.1/tabs.js"></script>
    <script src="libs/bs3compat-0.3.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
  <style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
    
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">数据科学导论（Introductory of Data Science）</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="有监督学习概论" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> 有监督学习概论</h1>
<div id="机器学习" class="section level3" number="5.0.1">
<h3><span class="header-section-number">5.0.1</span> 机器学习</h3>
<ul>
<li>机器学习1959年由Arthur Samuel首次提出；</li>
<li>机器学习是研究计算机算法，这个算法可通过数据产生的经验自动地改善（Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data）（Mitchell,Tom，1959）</li>
<li>机器学习是计算机程序，如果该程序在T类任务中的表现，如P所衡量的，随着经验E的增加而提高，那么就可以说它从经验E中学习到了一些任务T和性能指标P。（A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.）（Tom M. Mitchell ）</li>
<li>一系列的数据驱动的算法和工具，它的主要目的是根据观测到的数据来预测没有观测到的数据，它的最大特点就是数据驱动。</li>
<li>机器学习的种类:</li>
</ul>
<div class="figure">
<img src="pics/machine%20learning.jpg" alt="" />
<p class="caption">machine learning</p>
</div>
</div>
<div id="机器学习实例" class="section level3" number="5.0.2">
<h3><span class="header-section-number">5.0.2</span> 机器学习实例</h3>
<ul>
<li><p>房价预测：如何对房子的价格进行预测？采集与房子有关的变量：区域、房子朝向、楼层、周围是否有地铁、面积、房屋的价格等数据，构建一个回归模型来进行预测。<span class="math display">\[y=f(X)+\epsilon\]</span></p></li>
<li><p>垃圾邮件分类：根据邮件内容提取出一些特征关键词作为X，而响应变量y为两个是值：1垃圾邮件，0好邮件，构建一个分类器进行邮件分类。<span class="math display">\[C(x)=P(y=1|X)\]</span></p></li>
<li><p>我们做过的舆情分析案例（聚类分析）：使用评论数据，通过词向量技术解析得到词向量，然后进行主题分析，得到网络谣言、疫情趋势、地点（国外）、处罚、疫苗等主题，从而根据这些主题词来识别谣言。</p></li>
<li><p>强化学习:玩Super Mario Bros游戏</p></li>
</ul>
<div class="figure">
<img src="pics/reinforcementLearning.png" alt="" />
<p class="caption">ReinforcementLearning</p>
</div>
</div>
<div id="模型及拟合" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> 模型及拟合</h2>
<ol style="list-style-type: decimal">
<li>模型：人们对所关心的真实世界问题的一个近似描述。</li>
</ol>
<div class="figure">
<img src="pics/model.jpg" alt="" />
<p class="caption">Model</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li><p>决策树模型：是一种有监督学习的算法，使用一系列的是和否规则（if-then rules)来进行决策，和人进行决策的过程类似。
<img src="pics/decisiontree.jpg" alt="Model" /></p>
<ul>
<li>数据：</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="有监督学习概论.html#cb497-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages{&quot;rpart&quot;}</span></span>
<span id="cb497-2"><a href="有监督学习概论.html#cb497-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages{&quot;rattle&quot;}</span></span>
<span id="cb497-3"><a href="有监督学习概论.html#cb497-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages{&quot;partykit&quot;}</span></span>
<span id="cb497-4"><a href="有监督学习概论.html#cb497-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb497-5"><a href="有监督学习概论.html#cb497-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb497-6"><a href="有监督学习概论.html#cb497-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rattle)</span></code></pre></div>
<pre><code>## Loading required package: tibble</code></pre>
<pre><code>## Loading required package: bitops</code></pre>
<pre><code>## Rattle: A free graphical interface for data science with R.
## Version 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.
## Type &#39;rattle()&#39; to shake, rattle, and roll your data.</code></pre>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="有监督学习概论.html#cb501-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(partykit)</span></code></pre></div>
<pre><code>## Loading required package: grid</code></pre>
<pre><code>## Loading required package: libcoin</code></pre>
<pre><code>## Loading required package: mvtnorm</code></pre>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="有监督学习概论.html#cb505-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inpput the file</span></span>
<span id="cb505-2"><a href="有监督学习概论.html#cb505-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(trees)</span>
<span id="cb505-3"><a href="有监督学习概论.html#cb505-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(trees)</span></code></pre></div>
<pre><code>##   Girth Height Volume
## 1   8.3     70   10.3
## 2   8.6     65   10.3
## 3   8.8     63   10.2
## 4  10.5     72   16.4
## 5  10.7     81   18.8
## 6  10.8     83   19.7</code></pre>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="有监督学习概论.html#cb507-1" aria-hidden="true" tabindex="-1"></a>volume <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Volume <span class="sc">~</span> ., <span class="at">data =</span> trees)</span>
<span id="cb507-2"><a href="有监督学习概论.html#cb507-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(volume, <span class="at">type =</span> <span class="dv">3</span>, <span class="at">clip.right.labs =</span> <span class="cn">FALSE</span>, <span class="at">branch =</span> .<span class="dv">3</span>, <span class="at">under =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="04-application_files/figure-html/decisiontree-1.png" width="672" /></p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="有监督学习概论.html#cb508-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;./data/simple4.csv&quot;</span>)</span>
<span id="cb508-2"><a href="有监督学习概论.html#cb508-2" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>##    sex haircol skirt style
## 1    F  yellow     y     n
## 2    F   black     y     n
## 3    F  yellow     n     y
## 4    F   black     n     y
## 5    M  yellow     n     n
## 6    M   black     n     n
## 7    M  yellow     y     n
## 8    F   black     n     y
## 9    M  yellow     n     n
## 10   F   black     y     n
## 11   M   black     y     y
## 12   F  yellow     n     y</code></pre>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="有监督学习概论.html#cb510-1" aria-hidden="true" tabindex="-1"></a><span class="co">#sex &lt;- factor(df$sex, levels = 0:1, labels = c(&quot;M&quot;, &quot;F&quot;))</span></span>
<span id="cb510-2"><a href="有监督学习概论.html#cb510-2" aria-hidden="true" tabindex="-1"></a><span class="co">#haircol &lt;- factor(df$haircol,levels = 0:1, labels = c(&quot;yellow&quot;, &quot;black&quot;))</span></span>
<span id="cb510-3"><a href="有监督学习概论.html#cb510-3" aria-hidden="true" tabindex="-1"></a><span class="co">#skirt &lt;- factor(df$skirt,levels = 0:1, labels = c(&quot;y&quot;, &quot;n&quot;))</span></span>
<span id="cb510-4"><a href="有监督学习概论.html#cb510-4" aria-hidden="true" tabindex="-1"></a><span class="co">#style &lt;- factor(df$style,levels = 0:1, labels = c(&quot;y&quot;, &quot;n&quot;))</span></span>
<span id="cb510-5"><a href="有监督学习概论.html#cb510-5" aria-hidden="true" tabindex="-1"></a><span class="co">#df[] &lt;- lapply(df,factor)</span></span>
<span id="cb510-6"><a href="有监督学习概论.html#cb510-6" aria-hidden="true" tabindex="-1"></a><span class="co">#str(df)</span></span>
<span id="cb510-7"><a href="有监督学习概论.html#cb510-7" aria-hidden="true" tabindex="-1"></a><span class="co">#output.tree&lt;- rpart(sex~.,data=df,cp=0.02)</span></span>
<span id="cb510-8"><a href="有监督学习概论.html#cb510-8" aria-hidden="true" tabindex="-1"></a><span class="co">#print(output.tree)</span></span>
<span id="cb510-9"><a href="有监督学习概论.html#cb510-9" aria-hidden="true" tabindex="-1"></a><span class="co">#rpart.plot(output.tree)</span></span></code></pre></div>
<p>3.节点纯度的度量：Gini系数和熵
+ 也称Gini不纯度，当随机选择一个特征，这个特定特征的总概率的计算（Gini Index, also known as Gini impurity, calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly）.
<span class="math display">\[GiniIndex=1-\sum\limits_{i=1}^kp_i^2\]</span>
+ 是数据不纯度或随机性的测度（it is the measurement of the impurity or randomness in the data points）.
<span class="math display">\[Entropy=-\sum\limits_{i=1}^kp_i\times log(p_i)\]</span>
+ 不同的决策树算法使用不同的计算方法，CART树使用Gini系数，ID3和C4.5算法使用熵。（Different decision tree algorithms utilize different impurity metrics: CART uses Gini; ID3 and C4.5 use Entropy).</p>
<p>4.决策树例子</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="有监督学习概论.html#cb511-1" aria-hidden="true" tabindex="-1"></a>decisiontree <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">emotion=</span><span class="fu">c</span>(<span class="st">&#39;sick&#39;</span>,<span class="st">&#39;sick&#39;</span>,<span class="st">&#39;sick&#39;</span>,<span class="st">&#39;notsick&#39;</span>,<span class="st">&#39;notsick&#39;</span>,<span class="st">&#39;sick&#39;</span>,<span class="st">&#39;notsick&#39;</span>,<span class="st">&#39;notsick&#39;</span>),</span>
<span id="cb511-2"><a href="有监督学习概论.html#cb511-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">temperature=</span><span class="fu">c</span>(<span class="st">&#39;under&#39;</span>,<span class="st">&#39;over&#39;</span>,<span class="st">&#39;under&#39;</span>,<span class="st">&#39;under&#39;</span>,<span class="st">&#39;over&#39;</span>,<span class="st">&#39;over&#39;</span>,<span class="st">&#39;under&#39;</span>,<span class="st">&#39;over&#39;</span>),</span>
<span id="cb511-3"><a href="有监督学习概论.html#cb511-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">stayhome=</span><span class="fu">c</span>(<span class="st">&#39;N&#39;</span>,<span class="st">&#39;Y&#39;</span>,<span class="st">&#39;Y&#39;</span>,<span class="st">&#39;N&#39;</span>,<span class="st">&#39;Y&#39;</span>,<span class="st">&#39;N&#39;</span>,<span class="st">&#39;N&#39;</span>,<span class="st">&#39;Y&#39;</span>))</span>
<span id="cb511-4"><a href="有监督学习概论.html#cb511-4" aria-hidden="true" tabindex="-1"></a>decisiontree</span></code></pre></div>
<pre><code>##   emotion temperature stayhome
## 1    sick       under        N
## 2    sick        over        Y
## 3    sick       under        Y
## 4 notsick       under        N
## 5 notsick        over        Y
## 6    sick        over        N
## 7 notsick       under        N
## 8 notsick        over        Y</code></pre>
<ul>
<li><p>计算步骤：</p>
<ul>
<li>选择stayhome作为因变量y,emotion和temperature作为自变量<span class="math inline">\(x_1,x_2\)</span>;</li>
<li>首先计算根结点的GiniIndex：<span class="math inline">\(GiniIndex_r=\frac{1}{2}\)</span>;</li>
<li>随机选择一个变量，如<span class="math inline">\(x_2\)</span>进行根结点裂分，分别计算左节点(condition为True)的Gini系数<span class="math inline">\(GiniIndex_1=\frac{3}{8}\)</span>和右节点(condition为False)的Gini系数<span class="math inline">\(GiniIndex_2=\frac{3}{8}\)</span>，然后计算加权的GiniIndex<span class="math inline">\(GiniIndex_w=\frac{3}{8}\times\frac{4}{8}+\frac{3}{8}\times\frac{4}{8}=\frac{1}{2}\)</span>,最后计算使用<span class="math inline">\(x_2\)</span>进行根结点裂分得到的基尼不纯度的降低值<span class="math inline">\(GiniIndex_r-GiniIndex_w=\frac{1}{2}-\frac{3}{8}=\frac{1}{8}\)</span>;</li>
<li>根据上述过程选择另一个变量(<span class="math inline">\(x_1\)</span>)，进行类似的计算得到的基尼不纯度的降低值为0</li>
<li>所以第一轮选择<span class="math inline">\(x_2\)</span>进行裂分。</li>
</ul></li>
<li><p>使用信息熵的方法进行相似的计算。</p></li>
</ul>
<p>5.预测值、拟合值及误判率</p>
<ul>
<li>如果用训练得到的模型作用于一个(也有这几个自变量的) 数据, 那么就会得到预测值(predicted value).</li>
<li>对于训练模型的训练集做预测所得到的预测值也称为拟合值(fitted value).</li>
<li>使用训练模型的训练集会产生一个误判, 这实际上是用训练集“学习到” 的模型对训练集的因变量sex做预测。</li>
<li>混淆矩阵(confusion Matrix)是机器学习用来衡量分类好坏的方法（Confusion Matrix is a performance measurement for machine learning classification）.</li>
</ul>
<p><img src="pics/confusionmatrix.png" alt="Confusion Matrix" />
6.过拟合</p>
<ul>
<li>机器学习的目的是得到一个泛化误差比较小的模型；</li>
<li>如果用训练集所得到的误判率和用非训练集(或测试集) 得到的误判率之间差别很大, 则说明该模型有过拟合现象(overfitting).</li>
<li>在大数据时代，过拟合是模型构建过程中最易出现的情况，得到的模型是一个看上去很好，而实际上无用的模型。</li>
<li>简单来说就是模型在训练集上得到的误差非常小（偏差较小）而在不同数据集之间得到的误差非常大（方差较大），而模型总的误差基本由偏差和方差组成。</li>
<li>例子：买家秀和卖家秀；</li>
</ul>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="有监督学习概论.html#cb513-1" aria-hidden="true" tabindex="-1"></a><span class="do">###Overfitting</span></span>
<span id="cb513-2"><a href="有监督学习概论.html#cb513-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(data.table)</span></code></pre></div>
<pre><code>## Loading required package: data.table</code></pre>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="有监督学习概论.html#cb515-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb515-2"><a href="有监督学习概论.html#cb515-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span></code></pre></div>
<pre><code>## Loading required package: ggplot2</code></pre>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="有监督学习概论.html#cb517-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb517-2"><a href="有监督学习概论.html#cb517-2" aria-hidden="true" tabindex="-1"></a><span class="do">##Reading data</span></span>
<span id="cb517-3"><a href="有监督学习概论.html#cb517-3" aria-hidden="true" tabindex="-1"></a>overfitting_data<span class="ot">=</span><span class="fu">data.table</span>(airquality)</span>
<span id="cb517-4"><a href="有监督学习概论.html#cb517-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(overfitting_data,<span class="fu">aes</span>(Wind,Ozone))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span><span class="fu">ggtitle</span>(<span class="st">&quot;Ozone vs wind speed&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: Removed 37 rows containing missing values (geom_point).</code></pre>
<p><img src="04-application_files/figure-html/overfitting-1.png" width="672" /></p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="有监督学习概论.html#cb519-1" aria-hidden="true" tabindex="-1"></a>data_test<span class="ot">=</span><span class="fu">na.omit</span>(overfitting_data[,.(Wind,Ozone)])</span>
<span id="cb519-2"><a href="有监督学习概论.html#cb519-2" aria-hidden="true" tabindex="-1"></a>train_sample<span class="ot">=</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data_test),<span class="at">size =</span> <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(data_test))</span>
<span id="cb519-3"><a href="有监督学习概论.html#cb519-3" aria-hidden="true" tabindex="-1"></a><span class="do">###creation of polynomial models</span></span>
<span id="cb519-4"><a href="有监督学习概论.html#cb519-4" aria-hidden="true" tabindex="-1"></a>degree_of_poly<span class="ot">=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">20</span></span>
<span id="cb519-5"><a href="有监督学习概论.html#cb519-5" aria-hidden="true" tabindex="-1"></a>degree_to_plot<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>)</span>
<span id="cb519-6"><a href="有监督学习概论.html#cb519-6" aria-hidden="true" tabindex="-1"></a>polynomial_model<span class="ot">=</span><span class="fu">list</span>()</span>
<span id="cb519-7"><a href="有监督学习概论.html#cb519-7" aria-hidden="true" tabindex="-1"></a>df_result<span class="ot">=</span><span class="cn">NULL</span></span>
<span id="cb519-8"><a href="有监督学习概论.html#cb519-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (degree <span class="cf">in</span> degree_of_poly)</span>
<span id="cb519-9"><a href="有监督学习概论.html#cb519-9" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb519-10"><a href="有监督学习概论.html#cb519-10" aria-hidden="true" tabindex="-1"></a> fm<span class="ot">=</span><span class="fu">as.formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;Ozone~poly(Wind,&quot;</span>,degree,<span class="st">&quot;,raw=T)&quot;</span>))</span>
<span id="cb519-11"><a href="有监督学习概论.html#cb519-11" aria-hidden="true" tabindex="-1"></a> polynomial_model<span class="ot">=</span><span class="fu">c</span>(polynomial_model,<span class="fu">list</span>(<span class="fu">lm</span>(fm,data_test[train_sample])))</span>
<span id="cb519-12"><a href="有监督学习概论.html#cb519-12" aria-hidden="true" tabindex="-1"></a> Polynomial_degree<span class="ot">=</span><span class="fu">paste0</span>(degree)</span>
<span id="cb519-13"><a href="有监督学习概论.html#cb519-13" aria-hidden="true" tabindex="-1"></a> data_fitted<span class="ot">=</span><span class="fu">tail</span>(polynomial_model,<span class="dv">1</span>)[[<span class="dv">1</span>]]<span class="sc">$</span>fitted.values</span>
<span id="cb519-14"><a href="有监督学习概论.html#cb519-14" aria-hidden="true" tabindex="-1"></a> new_df<span class="ot">=</span><span class="fu">data.table</span>(<span class="at">Wind=</span>data_test[train_sample,Wind],<span class="at">Ozone_real=</span>data_test[train_sample,Ozone],<span class="at">Ozone_fitted=</span><span class="fu">tail</span>(polynomial_model,<span class="dv">1</span>)[[<span class="dv">1</span>]]<span class="sc">$</span>fitted.values,<span class="at">degree=</span><span class="fu">as.factor</span>(degree))</span>
<span id="cb519-15"><a href="有监督学习概论.html#cb519-15" aria-hidden="true" tabindex="-1"></a> <span class="cf">if</span> (<span class="fu">is.null</span>(df_result))</span>
<span id="cb519-16"><a href="有监督学习概论.html#cb519-16" aria-hidden="true" tabindex="-1"></a> df_result<span class="ot">=</span>new_df</span>
<span id="cb519-17"><a href="有监督学习概论.html#cb519-17" aria-hidden="true" tabindex="-1"></a> <span class="cf">else</span></span>
<span id="cb519-18"><a href="有监督学习概论.html#cb519-18" aria-hidden="true" tabindex="-1"></a> df_result<span class="ot">=</span><span class="fu">rbind</span>(df_result,new_df)</span>
<span id="cb519-19"><a href="有监督学习概论.html#cb519-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb519-20"><a href="有监督学习概论.html#cb519-20" aria-hidden="true" tabindex="-1"></a>gg<span class="ot">=</span><span class="fu">ggplot</span>(df_result[degree<span class="sc">%in%</span>degree_to_plot],<span class="fu">aes</span>(<span class="at">x=</span>Wind))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y=</span>Ozone_real))<span class="sc">+</span><span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color=</span>degree,<span class="at">y=</span>Ozone_fitted))</span>
<span id="cb519-21"><a href="有监督学习概论.html#cb519-21" aria-hidden="true" tabindex="-1"></a>gg<span class="sc">+</span><span class="fu">ggtitle</span>(<span class="st">&#39;Ozone vs wind for several polynomial regressions&#39;</span>)<span class="sc">+</span><span class="fu">ylab</span>(<span class="st">&#39;Ozone&#39;</span>)</span></code></pre></div>
<p><img src="04-application_files/figure-html/overfitting-2.png" width="672" /></p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="有监督学习概论.html#cb520-1" aria-hidden="true" tabindex="-1"></a><span class="do">###Computing SE</span></span>
<span id="cb520-2"><a href="有监督学习概论.html#cb520-2" aria-hidden="true" tabindex="-1"></a>SE_train_list<span class="ot">=</span><span class="fu">c</span>()</span>
<span id="cb520-3"><a href="有监督学习概论.html#cb520-3" aria-hidden="true" tabindex="-1"></a>SE_test_list<span class="ot">=</span><span class="fu">c</span>()</span>
<span id="cb520-4"><a href="有监督学习概论.html#cb520-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (poly_mod <span class="cf">in</span> polynomial_model)</span>
<span id="cb520-5"><a href="有监督学习概论.html#cb520-5" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb520-6"><a href="有监督学习概论.html#cb520-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">print</span>(<span class="fu">summary</span>(poly_mod))</span>
<span id="cb520-7"><a href="有监督学习概论.html#cb520-7" aria-hidden="true" tabindex="-1"></a> SE_train_list<span class="ot">=</span><span class="fu">c</span>(SE_train_list,<span class="fu">sqrt</span>(<span class="fu">mean</span>(poly_mod<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb520-8"><a href="有监督学习概论.html#cb520-8" aria-hidden="true" tabindex="-1"></a> SE_test<span class="ot">=</span><span class="fu">sqrt</span>(<span class="fu">mean</span>((data_test[<span class="sc">-</span>train_sample]<span class="sc">-</span><span class="fu">predict</span>(poly_mod,data_test[<span class="sc">-</span>train_sample,]))<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb520-9"><a href="有监督学习概论.html#cb520-9" aria-hidden="true" tabindex="-1"></a> SE_test_list<span class="ot">=</span><span class="fu">c</span>(SE_test_list,SE_test)</span>
<span id="cb520-10"><a href="有监督学习概论.html#cb520-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.713 -17.792  -3.344  15.472  60.929 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             98.0237     8.4012   11.67  &lt; 2e-16 ***
## poly(Wind, 1, raw = T)  -5.8422     0.7842   -7.45 1.02e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 24.64 on 79 degrees of freedom
## Multiple R-squared:  0.4127, Adjusted R-squared:  0.4052 
## F-statistic:  55.5 on 1 and 79 DF,  p-value: 1.019e-10</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -42.19 -14.61  -4.92  12.24  60.33 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             157.5771    16.0821   9.798 3.08e-15 ***
## poly(Wind, 2, raw = T)1 -18.5808     3.1105  -5.974 6.52e-08 ***
## poly(Wind, 2, raw = T)2   0.6053     0.1439   4.207 6.85e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22.39 on 78 degrees of freedom
## Multiple R-squared:  0.5213, Adjusted R-squared:  0.509 
## F-statistic: 42.47 on 2 and 78 DF,  p-value: 3.334e-13</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.103 -13.038  -3.920   9.021  63.362 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             194.34698   30.01711   6.475 8.07e-09 ***
## poly(Wind, 3, raw = T)1 -31.63124    9.53487  -3.317  0.00139 ** 
## poly(Wind, 3, raw = T)2   1.94131    0.93449   2.077  0.04110 *  
## poly(Wind, 3, raw = T)3  -0.04083    0.02822  -1.447  0.15203    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22.23 on 77 degrees of freedom
## Multiple R-squared:  0.534,  Adjusted R-squared:  0.5158 
## F-statistic: 29.41 on 3 and 77 DF,  p-value: 8.961e-13</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.262 -13.526  -3.561  10.257  62.494 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)             126.319188  52.823584   2.391   0.0193 *
## poly(Wind, 4, raw = T)1   3.550616  24.473809   0.145   0.8850  
## poly(Wind, 4, raw = T)2  -3.918866   3.872884  -1.012   0.3148  
## poly(Wind, 4, raw = T)3   0.346829   0.250332   1.385   0.1700  
## poly(Wind, 4, raw = T)4  -0.008786   0.005638  -1.558   0.1233  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22.03 on 76 degrees of freedom
## Multiple R-squared:  0.5484, Adjusted R-squared:  0.5246 
## F-statistic: 23.07 on 4 and 76 DF,  p-value: 1.661e-12</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -41.18 -12.81  -3.65  10.19  66.73 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)             -47.642955 101.574573  -0.469   0.6404  
## poly(Wind, 5, raw = T)1 124.900947  65.499114   1.907   0.0604 .
## poly(Wind, 5, raw = T)2 -33.043592  15.111328  -2.187   0.0319 *
## poly(Wind, 5, raw = T)3   3.465855   1.585451   2.186   0.0319 *
## poly(Wind, 5, raw = T)4  -0.161312   0.076795  -2.101   0.0390 *
## poly(Wind, 5, raw = T)5   0.002768   0.001390   1.991   0.0501 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.61 on 75 degrees of freedom
## Multiple R-squared:  0.5711, Adjusted R-squared:  0.5425 
## F-statistic: 19.97 on 5 and 75 DF,  p-value: 1.322e-12</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.219 -13.286  -4.391   9.609  67.543 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)             -1.226e+02  1.936e+02  -0.633    0.528
## poly(Wind, 6, raw = T)1  1.884e+02  1.541e+02   1.222    0.225
## poly(Wind, 6, raw = T)2 -5.271e+01  4.573e+01  -1.153    0.253
## poly(Wind, 6, raw = T)3  6.398e+00  6.626e+00   0.966    0.337
## poly(Wind, 6, raw = T)4 -3.880e-01  5.031e-01  -0.771    0.443
## poly(Wind, 6, raw = T)5  1.150e-02  1.919e-02   0.599    0.551
## poly(Wind, 6, raw = T)6 -1.321e-04  2.897e-04  -0.456    0.650
## 
## Residual standard error: 21.73 on 74 degrees of freedom
## Multiple R-squared:  0.5723, Adjusted R-squared:  0.5376 
## F-statistic:  16.5 on 6 and 74 DF,  p-value: 5.696e-12</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.181 -13.217  -4.417   9.583  67.517 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)             -1.308e+02  4.205e+02  -0.311    0.757
## poly(Wind, 7, raw = T)1  1.966e+02  4.059e+02   0.484    0.630
## poly(Wind, 7, raw = T)2 -5.585e+01  1.516e+02  -0.368    0.714
## poly(Wind, 7, raw = T)3  7.010e+00  2.892e+01   0.242    0.809
## poly(Wind, 7, raw = T)4 -4.540e-01  3.080e+00  -0.147    0.883
## poly(Wind, 7, raw = T)5  1.550e-02  1.850e-01   0.084    0.933
## poly(Wind, 7, raw = T)6 -2.590e-04  5.844e-03  -0.044    0.965
## poly(Wind, 7, raw = T)7  1.641e-06  7.546e-05   0.022    0.983
## 
## Residual standard error: 21.88 on 73 degrees of freedom
## Multiple R-squared:  0.5723, Adjusted R-squared:  0.5313 
## F-statistic: 13.95 on 7 and 73 DF,  p-value: 2.446e-11</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.324 -13.372  -5.533   8.916  69.196 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)              4.254e+02  9.194e+02   0.463    0.645
## poly(Wind, 8, raw = T)1 -4.396e+02  1.019e+03  -0.431    0.668
## poly(Wind, 8, raw = T)2  2.331e+02  4.508e+02   0.517    0.607
## poly(Wind, 8, raw = T)3 -6.202e+01  1.055e+02  -0.588    0.558
## poly(Wind, 8, raw = T)4  9.144e+00  1.443e+01   0.634    0.528
## poly(Wind, 8, raw = T)5 -7.876e-01  1.194e+00  -0.660    0.512
## poly(Wind, 8, raw = T)6  3.953e-02  5.873e-02   0.673    0.503
## poly(Wind, 8, raw = T)7 -1.073e-03  1.579e-03  -0.679    0.499
## poly(Wind, 8, raw = T)8  1.216e-05  1.786e-05   0.681    0.498
## 
## Residual standard error: 21.96 on 72 degrees of freedom
## Multiple R-squared:  0.575,  Adjusted R-squared:  0.5278 
## F-statistic: 12.18 on 8 and 72 DF,  p-value: 7.728e-11</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.191 -14.464  -5.251   8.702  69.369 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)              2.187e+03  1.881e+03   1.162    0.249
## poly(Wind, 9, raw = T)1 -2.722e+03  2.358e+03  -1.154    0.252
## poly(Wind, 9, raw = T)2  1.443e+03  1.215e+03   1.188    0.239
## poly(Wind, 9, raw = T)3 -4.100e+02  3.411e+02  -1.202    0.233
## poly(Wind, 9, raw = T)4  6.943e+01  5.801e+01   1.197    0.235
## poly(Wind, 9, raw = T)5 -7.358e+00  6.239e+00  -1.179    0.242
## poly(Wind, 9, raw = T)6  4.928e-01  4.265e-01   1.155    0.252
## poly(Wind, 9, raw = T)7 -2.026e-02  1.796e-02  -1.128    0.263
## poly(Wind, 9, raw = T)8  4.667e-04  4.241e-04   1.101    0.275
## poly(Wind, 9, raw = T)9 -4.609e-06  4.296e-06  -1.073    0.287
## 
## Residual standard error: 21.93 on 71 degrees of freedom
## Multiple R-squared:  0.5818, Adjusted R-squared:  0.5288 
## F-statistic: 10.97 on 9 and 71 DF,  p-value: 1.659e-10</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -36.654 -13.654  -4.266   8.975  66.521 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                5.719e+03  3.949e+03   1.448    0.152
## poly(Wind, 10, raw = T)1  -7.913e+03  5.623e+03  -1.407    0.164
## poly(Wind, 10, raw = T)2   4.640e+03  3.369e+03   1.377    0.173
## poly(Wind, 10, raw = T)3  -1.501e+03  1.126e+03  -1.334    0.187
## poly(Wind, 10, raw = T)4   2.994e+02  2.334e+02   1.283    0.204
## poly(Wind, 10, raw = T)5  -3.879e+01  3.153e+01  -1.230    0.223
## poly(Wind, 10, raw = T)6   3.330e+00  2.822e+00   1.180    0.242
## poly(Wind, 10, raw = T)7  -1.880e-01  1.659e-01  -1.133    0.261
## poly(Wind, 10, raw = T)8   6.713e-03  6.157e-03   1.090    0.279
## poly(Wind, 10, raw = T)9  -1.373e-04  1.306e-04  -1.052    0.297
## poly(Wind, 10, raw = T)10  1.226e-06  1.206e-06   1.017    0.313
## 
## Residual standard error: 21.93 on 70 degrees of freedom
## Multiple R-squared:  0.5879, Adjusted R-squared:  0.529 
## F-statistic: 9.985 on 10 and 70 DF,  p-value: 3.571e-10</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.409 -13.059  -3.181   5.338  64.988 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)                1.606e+04  7.917e+03   2.028   0.0464 *
## poly(Wind, 11, raw = T)1  -2.490e+04  1.260e+04  -1.975   0.0522 .
## poly(Wind, 11, raw = T)2   1.653e+04  8.591e+03   1.924   0.0584 .
## poly(Wind, 11, raw = T)3  -6.203e+03  3.322e+03  -1.867   0.0661 .
## poly(Wind, 11, raw = T)4   1.471e+03  8.131e+02   1.809   0.0749 .
## poly(Wind, 11, raw = T)5  -2.327e+02  1.328e+02  -1.752   0.0841 .
## poly(Wind, 11, raw = T)6   2.519e+01  1.481e+01   1.700   0.0936 .
## poly(Wind, 11, raw = T)7  -1.873e+00  1.133e+00  -1.652   0.1030  
## poly(Wind, 11, raw = T)8   9.407e-02  5.846e-02   1.609   0.1122  
## poly(Wind, 11, raw = T)9  -3.048e-03  1.942e-03  -1.570   0.1210  
## poly(Wind, 11, raw = T)10  5.750e-05  3.747e-05   1.534   0.1295  
## poly(Wind, 11, raw = T)11 -4.793e-07  3.190e-07  -1.502   0.1375  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.73 on 69 degrees of freedom
## Multiple R-squared:  0.6009, Adjusted R-squared:  0.5373 
## F-statistic: 9.446 on 11 and 69 DF,  p-value: 4.25e-10</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.409 -13.059  -3.181   5.338  64.988 
## 
## Coefficients: (1 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)                1.606e+04  7.917e+03   2.028   0.0464 *
## poly(Wind, 12, raw = T)1  -2.490e+04  1.260e+04  -1.975   0.0522 .
## poly(Wind, 12, raw = T)2   1.653e+04  8.591e+03   1.924   0.0584 .
## poly(Wind, 12, raw = T)3  -6.203e+03  3.322e+03  -1.867   0.0661 .
## poly(Wind, 12, raw = T)4   1.471e+03  8.131e+02   1.809   0.0749 .
## poly(Wind, 12, raw = T)5  -2.327e+02  1.328e+02  -1.752   0.0841 .
## poly(Wind, 12, raw = T)6   2.519e+01  1.481e+01   1.700   0.0936 .
## poly(Wind, 12, raw = T)7  -1.873e+00  1.133e+00  -1.652   0.1030  
## poly(Wind, 12, raw = T)8   9.407e-02  5.846e-02   1.609   0.1122  
## poly(Wind, 12, raw = T)9  -3.048e-03  1.942e-03  -1.570   0.1210  
## poly(Wind, 12, raw = T)10  5.750e-05  3.747e-05   1.534   0.1295  
## poly(Wind, 12, raw = T)11 -4.793e-07  3.190e-07  -1.502   0.1375  
## poly(Wind, 12, raw = T)12         NA         NA      NA       NA  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.73 on 69 degrees of freedom
## Multiple R-squared:  0.6009, Adjusted R-squared:  0.5373 
## F-statistic: 9.446 on 11 and 69 DF,  p-value: 4.25e-10</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## Warning in mean.default((data_test[-train_sample] - predict(poly_mod,
## data_test[-train_sample, : argument is not numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.775 -13.102  -3.242   5.758  65.355 
## 
## Coefficients: (1 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                1.848e+04  1.953e+04   0.946    0.347
## poly(Wind, 13, raw = T)1  -2.924e+04  3.441e+04  -0.850    0.398
## poly(Wind, 13, raw = T)2   1.990e+04  2.623e+04   0.759    0.451
## poly(Wind, 13, raw = T)3  -7.692e+03  1.146e+04  -0.671    0.504
## poly(Wind, 13, raw = T)4   1.892e+03  3.207e+03   0.590    0.557
## poly(Wind, 13, raw = T)5  -3.131e+02  6.067e+02  -0.516    0.608
## poly(Wind, 13, raw = T)6   3.583e+01  7.974e+01   0.449    0.655
## poly(Wind, 13, raw = T)7  -2.858e+00  7.340e+00  -0.389    0.698
## poly(Wind, 13, raw = T)8   1.573e-01  4.687e-01   0.335    0.738
## poly(Wind, 13, raw = T)9  -5.764e-03  2.009e-02  -0.287    0.775
## poly(Wind, 13, raw = T)10  1.298e-04  5.336e-04   0.243    0.809
## poly(Wind, 13, raw = T)11 -1.435e-06  7.039e-06  -0.204    0.839
## poly(Wind, 13, raw = T)12         NA         NA      NA       NA
## poly(Wind, 13, raw = T)13  1.059e-10  7.791e-10   0.136    0.892
## 
## Residual standard error: 21.89 on 68 degrees of freedom
## Multiple R-squared:  0.601,  Adjusted R-squared:  0.5306 
## F-statistic: 8.537 on 12 and 68 DF,  p-value: 1.345e-09</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.775 -13.102  -3.242   5.758  65.355 
## 
## Coefficients: (2 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                1.848e+04  1.953e+04   0.946    0.347
## poly(Wind, 14, raw = T)1  -2.924e+04  3.441e+04  -0.850    0.398
## poly(Wind, 14, raw = T)2   1.990e+04  2.623e+04   0.759    0.451
## poly(Wind, 14, raw = T)3  -7.692e+03  1.146e+04  -0.671    0.504
## poly(Wind, 14, raw = T)4   1.892e+03  3.207e+03   0.590    0.557
## poly(Wind, 14, raw = T)5  -3.131e+02  6.067e+02  -0.516    0.608
## poly(Wind, 14, raw = T)6   3.583e+01  7.974e+01   0.449    0.655
## poly(Wind, 14, raw = T)7  -2.858e+00  7.340e+00  -0.389    0.698
## poly(Wind, 14, raw = T)8   1.573e-01  4.687e-01   0.335    0.738
## poly(Wind, 14, raw = T)9  -5.764e-03  2.009e-02  -0.287    0.775
## poly(Wind, 14, raw = T)10  1.298e-04  5.336e-04   0.243    0.809
## poly(Wind, 14, raw = T)11 -1.435e-06  7.039e-06  -0.204    0.839
## poly(Wind, 14, raw = T)12         NA         NA      NA       NA
## poly(Wind, 14, raw = T)13  1.059e-10  7.791e-10   0.136    0.892
## poly(Wind, 14, raw = T)14         NA         NA      NA       NA
## 
## Residual standard error: 21.89 on 68 degrees of freedom
## Multiple R-squared:  0.601,  Adjusted R-squared:  0.5306 
## F-statistic: 8.537 on 12 and 68 DF,  p-value: 1.345e-09</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.065 -12.815  -3.778   5.350  65.972 
## 
## Coefficients: (2 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                3.007e+04  4.803e+04   0.626    0.533
## poly(Wind, 15, raw = T)1  -5.152e+04  9.105e+04  -0.566    0.573
## poly(Wind, 15, raw = T)2   3.857e+04  7.539e+04   0.512    0.611
## poly(Wind, 15, raw = T)3  -1.675e+04  3.615e+04  -0.463    0.645
## poly(Wind, 15, raw = T)4   4.734e+03  1.122e+04   0.422    0.674
## poly(Wind, 15, raw = T)5  -9.226e+02  2.384e+03  -0.387    0.700
## poly(Wind, 15, raw = T)6   1.277e+02  3.566e+02   0.358    0.721
## poly(Wind, 15, raw = T)7  -1.270e+01  3.794e+01  -0.335    0.739
## poly(Wind, 15, raw = T)8   9.013e-01  2.852e+00   0.316    0.753
## poly(Wind, 15, raw = T)9  -4.433e-02  1.472e-01  -0.301    0.764
## poly(Wind, 15, raw = T)10  1.405e-03  4.852e-03   0.290    0.773
## poly(Wind, 15, raw = T)11 -2.327e-05  8.284e-05  -0.281    0.780
## poly(Wind, 15, raw = T)12         NA         NA      NA       NA
## poly(Wind, 15, raw = T)13  5.318e-09  1.972e-08   0.270    0.788
## poly(Wind, 15, raw = T)14         NA         NA      NA       NA
## poly(Wind, 15, raw = T)15 -1.260e-12  4.764e-12  -0.265    0.792
## 
## Residual standard error: 22.04 on 67 degrees of freedom
## Multiple R-squared:  0.6015, Adjusted R-squared:  0.5241 
## F-statistic: 7.778 on 13 and 67 DF,  p-value: 3.949e-09</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.065 -12.815  -3.778   5.350  65.972 
## 
## Coefficients: (3 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                3.007e+04  4.803e+04   0.626    0.533
## poly(Wind, 16, raw = T)1  -5.152e+04  9.105e+04  -0.566    0.573
## poly(Wind, 16, raw = T)2   3.857e+04  7.539e+04   0.512    0.611
## poly(Wind, 16, raw = T)3  -1.675e+04  3.615e+04  -0.463    0.645
## poly(Wind, 16, raw = T)4   4.734e+03  1.122e+04   0.422    0.674
## poly(Wind, 16, raw = T)5  -9.226e+02  2.384e+03  -0.387    0.700
## poly(Wind, 16, raw = T)6   1.277e+02  3.566e+02   0.358    0.721
## poly(Wind, 16, raw = T)7  -1.270e+01  3.794e+01  -0.335    0.739
## poly(Wind, 16, raw = T)8   9.013e-01  2.852e+00   0.316    0.753
## poly(Wind, 16, raw = T)9  -4.433e-02  1.472e-01  -0.301    0.764
## poly(Wind, 16, raw = T)10  1.405e-03  4.852e-03   0.290    0.773
## poly(Wind, 16, raw = T)11 -2.327e-05  8.284e-05  -0.281    0.780
## poly(Wind, 16, raw = T)12         NA         NA      NA       NA
## poly(Wind, 16, raw = T)13  5.318e-09  1.972e-08   0.270    0.788
## poly(Wind, 16, raw = T)14         NA         NA      NA       NA
## poly(Wind, 16, raw = T)15 -1.260e-12  4.764e-12  -0.265    0.792
## poly(Wind, 16, raw = T)16         NA         NA      NA       NA
## 
## Residual standard error: 22.04 on 67 degrees of freedom
## Multiple R-squared:  0.6015, Adjusted R-squared:  0.5241 
## F-statistic: 7.778 on 13 and 67 DF,  p-value: 3.949e-09</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.065 -12.815  -3.778   5.350  65.972 
## 
## Coefficients: (4 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                3.007e+04  4.803e+04   0.626    0.533
## poly(Wind, 17, raw = T)1  -5.152e+04  9.105e+04  -0.566    0.573
## poly(Wind, 17, raw = T)2   3.857e+04  7.539e+04   0.512    0.611
## poly(Wind, 17, raw = T)3  -1.675e+04  3.615e+04  -0.463    0.645
## poly(Wind, 17, raw = T)4   4.734e+03  1.122e+04   0.422    0.674
## poly(Wind, 17, raw = T)5  -9.226e+02  2.384e+03  -0.387    0.700
## poly(Wind, 17, raw = T)6   1.277e+02  3.566e+02   0.358    0.721
## poly(Wind, 17, raw = T)7  -1.270e+01  3.794e+01  -0.335    0.739
## poly(Wind, 17, raw = T)8   9.013e-01  2.852e+00   0.316    0.753
## poly(Wind, 17, raw = T)9  -4.433e-02  1.472e-01  -0.301    0.764
## poly(Wind, 17, raw = T)10  1.405e-03  4.852e-03   0.290    0.773
## poly(Wind, 17, raw = T)11 -2.327e-05  8.284e-05  -0.281    0.780
## poly(Wind, 17, raw = T)12         NA         NA      NA       NA
## poly(Wind, 17, raw = T)13  5.318e-09  1.972e-08   0.270    0.788
## poly(Wind, 17, raw = T)14         NA         NA      NA       NA
## poly(Wind, 17, raw = T)15 -1.260e-12  4.764e-12  -0.265    0.792
## poly(Wind, 17, raw = T)16         NA         NA      NA       NA
## poly(Wind, 17, raw = T)17         NA         NA      NA       NA
## 
## Residual standard error: 22.04 on 67 degrees of freedom
## Multiple R-squared:  0.6015, Adjusted R-squared:  0.5241 
## F-statistic: 7.778 on 13 and 67 DF,  p-value: 3.949e-09</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.476 -12.354  -3.806   5.584  65.707 
## 
## Coefficients: (4 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                8.313e+04  1.232e+05   0.675    0.502
## poly(Wind, 18, raw = T)1  -1.587e+05  2.467e+05  -0.643    0.522
## poly(Wind, 18, raw = T)2   1.339e+05  2.172e+05   0.616    0.540
## poly(Wind, 18, raw = T)3  -6.616e+04  1.117e+05  -0.593    0.555
## poly(Wind, 18, raw = T)4   2.144e+04  3.744e+04   0.573    0.569
## poly(Wind, 18, raw = T)5  -4.818e+03  8.661e+03  -0.556    0.580
## poly(Wind, 18, raw = T)6   7.723e+02  1.423e+03   0.543    0.589
## poly(Wind, 18, raw = T)7  -8.921e+01  1.679e+02  -0.531    0.597
## poly(Wind, 18, raw = T)8   7.379e+00  1.413e+01   0.522    0.603
## poly(Wind, 18, raw = T)9  -4.250e-01  8.266e-01  -0.514    0.609
## poly(Wind, 18, raw = T)10  1.589e-02  3.132e-02   0.507    0.614
## poly(Wind, 18, raw = T)11 -3.133e-04  6.252e-04  -0.501    0.618
## poly(Wind, 18, raw = T)12         NA         NA      NA       NA
## poly(Wind, 18, raw = T)13  1.074e-07  2.189e-07   0.490    0.625
## poly(Wind, 18, raw = T)14         NA         NA      NA       NA
## poly(Wind, 18, raw = T)15 -4.475e-11  9.304e-11  -0.481    0.632
## poly(Wind, 18, raw = T)16         NA         NA      NA       NA
## poly(Wind, 18, raw = T)17         NA         NA      NA       NA
## poly(Wind, 18, raw = T)18  3.106e-16  6.637e-16   0.468    0.641
## 
## Residual standard error: 22.17 on 66 degrees of freedom
## Multiple R-squared:  0.6028, Adjusted R-squared:  0.5185 
## F-statistic: 7.154 on 14 and 66 DF,  p-value: 1.035e-08</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.476 -12.354  -3.806   5.584  65.707 
## 
## Coefficients: (5 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                8.313e+04  1.232e+05   0.675    0.502
## poly(Wind, 19, raw = T)1  -1.587e+05  2.467e+05  -0.643    0.522
## poly(Wind, 19, raw = T)2   1.339e+05  2.172e+05   0.616    0.540
## poly(Wind, 19, raw = T)3  -6.616e+04  1.117e+05  -0.593    0.555
## poly(Wind, 19, raw = T)4   2.144e+04  3.744e+04   0.573    0.569
## poly(Wind, 19, raw = T)5  -4.818e+03  8.661e+03  -0.556    0.580
## poly(Wind, 19, raw = T)6   7.723e+02  1.423e+03   0.543    0.589
## poly(Wind, 19, raw = T)7  -8.921e+01  1.679e+02  -0.531    0.597
## poly(Wind, 19, raw = T)8   7.379e+00  1.413e+01   0.522    0.603
## poly(Wind, 19, raw = T)9  -4.250e-01  8.266e-01  -0.514    0.609
## poly(Wind, 19, raw = T)10  1.589e-02  3.132e-02   0.507    0.614
## poly(Wind, 19, raw = T)11 -3.133e-04  6.252e-04  -0.501    0.618
## poly(Wind, 19, raw = T)12         NA         NA      NA       NA
## poly(Wind, 19, raw = T)13  1.074e-07  2.189e-07   0.490    0.625
## poly(Wind, 19, raw = T)14         NA         NA      NA       NA
## poly(Wind, 19, raw = T)15 -4.475e-11  9.304e-11  -0.481    0.632
## poly(Wind, 19, raw = T)16         NA         NA      NA       NA
## poly(Wind, 19, raw = T)17         NA         NA      NA       NA
## poly(Wind, 19, raw = T)18  3.106e-16  6.637e-16   0.468    0.641
## poly(Wind, 19, raw = T)19         NA         NA      NA       NA
## 
## Residual standard error: 22.17 on 66 degrees of freedom
## Multiple R-squared:  0.6028, Adjusted R-squared:  0.5185 
## F-statistic: 7.154 on 14 and 66 DF,  p-value: 1.035e-08</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm, data = data_test[train_sample])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.476 -12.354  -3.806   5.584  65.707 
## 
## Coefficients: (6 not defined because of singularities)
##                             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                8.313e+04  1.232e+05   0.675    0.502
## poly(Wind, 20, raw = T)1  -1.587e+05  2.467e+05  -0.643    0.522
## poly(Wind, 20, raw = T)2   1.339e+05  2.172e+05   0.616    0.540
## poly(Wind, 20, raw = T)3  -6.616e+04  1.117e+05  -0.593    0.555
## poly(Wind, 20, raw = T)4   2.144e+04  3.744e+04   0.573    0.569
## poly(Wind, 20, raw = T)5  -4.818e+03  8.661e+03  -0.556    0.580
## poly(Wind, 20, raw = T)6   7.723e+02  1.423e+03   0.543    0.589
## poly(Wind, 20, raw = T)7  -8.921e+01  1.679e+02  -0.531    0.597
## poly(Wind, 20, raw = T)8   7.379e+00  1.413e+01   0.522    0.603
## poly(Wind, 20, raw = T)9  -4.250e-01  8.266e-01  -0.514    0.609
## poly(Wind, 20, raw = T)10  1.589e-02  3.132e-02   0.507    0.614
## poly(Wind, 20, raw = T)11 -3.133e-04  6.252e-04  -0.501    0.618
## poly(Wind, 20, raw = T)12         NA         NA      NA       NA
## poly(Wind, 20, raw = T)13  1.074e-07  2.189e-07   0.490    0.625
## poly(Wind, 20, raw = T)14         NA         NA      NA       NA
## poly(Wind, 20, raw = T)15 -4.475e-11  9.304e-11  -0.481    0.632
## poly(Wind, 20, raw = T)16         NA         NA      NA       NA
## poly(Wind, 20, raw = T)17         NA         NA      NA       NA
## poly(Wind, 20, raw = T)18  3.106e-16  6.637e-16   0.468    0.641
## poly(Wind, 20, raw = T)19         NA         NA      NA       NA
## poly(Wind, 20, raw = T)20         NA         NA      NA       NA
## 
## Residual standard error: 22.17 on 66 degrees of freedom
## Multiple R-squared:  0.6028, Adjusted R-squared:  0.5185 
## F-statistic: 7.154 on 14 and 66 DF,  p-value: 1.035e-08</code></pre>
<pre><code>## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): prediction from a
## rank-deficient fit may be misleading

## Warning in predict.lm(poly_mod, data_test[-train_sample, ]): argument is not
## numeric or logical: returning NA</code></pre>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="有监督学习概论.html#cb562-1" aria-hidden="true" tabindex="-1"></a>data_plot<span class="ot">=</span><span class="fu">data.table</span>(SE_test_list,SE_train_list,degree_of_poly)</span>
<span id="cb562-2"><a href="有监督学习概论.html#cb562-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_plot[degree_of_poly<span class="sc">&lt;=</span><span class="dv">8</span>])<span class="sc">+</span><span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>degree_of_poly,<span class="at">y=</span>SE_test_list),<span class="at">color=</span><span class="st">&#39;red&#39;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>degree_of_poly,<span class="at">y=</span>SE_train_list))<span class="sc">+</span><span class="fu">ylab</span>(<span class="st">&#39;MSE&#39;</span>)<span class="sc">+</span><span class="fu">xlab</span>(<span class="st">&#39;Degrees of polynomial&#39;</span>)</span></code></pre></div>
<pre><code>## Warning: Removed 8 row(s) containing missing values (geom_path).</code></pre>
<p><img src="04-application_files/figure-html/overfitting-3.png" width="672" /></p>
</div>
<div id="训练最小二乘线性回归模型的例子" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> 训练最小二乘线性回归模型的例子</h2>
<ul>
<li>回归(regression)：因变量(y)是连续型变量，自变量<span class="math inline">\(X\in R^p\)</span>是p维的连续型或离散型变量，一般求解<span class="math inline">\(E(y|X=x)\)</span>的模型。</li>
<li>线性回归模型(一般形式）,其中<span class="math inline">\(\epsilon\)</span>是服从均值为0，方差为<span class="math inline">\(\sigma^2\)</span>的正态分布的随机变量：<span class="math display">\[y=\beta_0+\beta_1\times x_1+,\cdots,+\beta_p\times x_p+\epsilon=\sum\limits_{i=0}^p\beta_i\times x_i+\epsilon=X\beta+\epsilon\]</span></li>
<li>一元线性模型(simple linear model,SLM):最简单的线性模型，只包含一个自变量。<span class="math display">\[y=\beta_0+\beta_1\times x_1+\epsilon\]</span></li>
</ul>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="有监督学习概论.html#cb564-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data come from the R basic package which related to the black cherry tree,Girth is diameter of tree, Height and Volume</span></span>
<span id="cb564-2"><a href="有监督学习概论.html#cb564-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;trees&quot;</span>)</span>
<span id="cb564-3"><a href="有监督学习概论.html#cb564-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(trees)</span></code></pre></div>
<pre><code>##   Girth Height Volume
## 1   8.3     70   10.3
## 2   8.6     65   10.3
## 3   8.8     63   10.2
## 4  10.5     72   16.4
## 5  10.7     81   18.8
## 6  10.8     83   19.7</code></pre>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="有监督学习概论.html#cb566-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Girth <span class="sc">~</span> Height, <span class="at">data =</span> trees, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb566-2"><a href="有监督学习概论.html#cb566-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb566-3"><a href="有监督学习概论.html#cb566-3" aria-hidden="true" tabindex="-1"></a><span class="co"># run the model</span></span>
<span id="cb566-4"><a href="有监督学习概论.html#cb566-4" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Girth<span class="sc">~</span>Height,<span class="at">data=</span>trees)</span>
<span id="cb566-5"><a href="有监督学习概论.html#cb566-5" aria-hidden="true" tabindex="-1"></a>fit</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Girth ~ Height, data = trees)
## 
## Coefficients:
## (Intercept)       Height  
##     -6.1884       0.2557</code></pre>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="有监督学习概论.html#cb568-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Girth ~ Height, data = trees)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2386 -1.9205 -0.0714  2.7450  4.5384 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) -6.18839    5.96020  -1.038  0.30772   
## Height       0.25575    0.07816   3.272  0.00276 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.728 on 29 degrees of freedom
## Multiple R-squared:  0.2697, Adjusted R-squared:  0.2445 
## F-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758</code></pre>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="有监督学习概论.html#cb570-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize the results</span></span>
<span id="cb570-2"><a href="有监督学习概论.html#cb570-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Girth<span class="sc">~</span>Height,<span class="at">data=</span>trees,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb570-3"><a href="有监督学习概论.html#cb570-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the fitted line</span></span>
<span id="cb570-4"><a href="有监督学习概论.html#cb570-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit,<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb570-5"><a href="有监督学习概论.html#cb570-5" aria-hidden="true" tabindex="-1"></a><span class="co">#quantile regression</span></span>
<span id="cb570-6"><a href="有监督学习概论.html#cb570-6" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;quantreg&quot;)</span></span>
<span id="cb570-7"><a href="有监督学习概论.html#cb570-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quantreg)</span></code></pre></div>
<pre><code>## Loading required package: SparseM</code></pre>
<pre><code>## 
## Attaching package: &#39;SparseM&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     backsolve</code></pre>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="有监督学习概论.html#cb574-1" aria-hidden="true" tabindex="-1"></a>fitq <span class="ot">&lt;-</span> <span class="fu">rq</span>(Girth<span class="sc">~</span>Height,<span class="at">data=</span>trees)</span>
<span id="cb574-2"><a href="有监督学习概论.html#cb574-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fitq,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb574-3"><a href="有监督学习概论.html#cb574-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;lm&quot;</span>, <span class="st">&quot;rq&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="04-application_files/figure-html/slm-1.png" width="672" /></p>
<ul>
<li><p>最小一乘回归思想:计算<span class="math inline">\(\frac{1}{n}\sum\limits_{i=1}^n(y_i-\hat{y_i})\)</span></p></li>
<li><p>最小二乘回归：计算MSE=<span class="math inline">\(\frac{1}{n}\sum\limits_{i=1}^n(y_i-\hat{y_i})^2\)</span></p></li>
</ul>
</div>
<div id="模型评价" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> 模型评价</h2>
<ul>
<li>把数据分成：训练集(training data)、验证集(validate data)和测试集(test data),训练集用来学习模型；验证集进行参数优化和调节；测试集对模型的好坏进行最终的判定（泛化能力的评价），优点是相对比较客观，大家认可；缺点是需要较多的数据（大多数竞赛网站都不提供测试集中y的数值）。</li>
<li>交叉验证（K折交叉验证）：数据即是训练集又是测试集，把数据分成k个子集，使用其中一个作为测试集，剩下的k-1个作为训练集，依次反复进行k次训练和测试，最后把所有测试集得到的预测值进行平均，计算得到一个平均的测试误差。优点：可充分的利用数据，缺点:可能得到的解不一定是最优解，但基本为一个次优解。</li>
<li>模拟数据</li>
<li>分类模型的预测精度：混淆矩阵；</li>
<li>回归问题交叉验证预测精度标准：
<ul>
<li>均方误差：<span class="math display">\[MSE=\frac{1}{n}\sum\limits_{i=1}^n(y_i-\hat{y_i})^2\]</span></li>
<li>均方误差平方根：<span class="math display">\[RMSE=\sqrt{MSE}=\sqrt{\frac{1}{n}\sum\limits_{i=1}^n(y_i-\hat{y_i})^2}\]</span></li>
<li>标准化均方误差: <span class="math display">\[NMSE=\frac{\sum\limits_{i=1}^n(y_i-\hat{y_i})^2}{\sum\limits_{i=1}^n(y_i-\bar{y_i})^2}\]</span></li>
<li>误差平均绝对值:<span class="math display">\[MAE=\frac{\sum\limits_{i=1}^n|y_i-\hat{y_i}|}{n}\]</span></li>
<li><span class="math inline">\(R^2\)</span>:<span class="math display">\[R^2=1-NMSE\]</span></li>
</ul></li>
</ul>
</div>
<div id="exercise" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Exercise</h2>

</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>数据科学导论（Introductory of Data Science）</strong>" was written by 潘蓄林. It was last built on 2022-06-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
